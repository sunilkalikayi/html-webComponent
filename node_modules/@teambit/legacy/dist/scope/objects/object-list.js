"use strict";

var _interopRequireDefault = require("@babel/runtime/helpers/interopRequireDefault");
require("core-js/modules/es.symbol.description.js");
require("core-js/modules/es.symbol.async-iterator.js");
require("core-js/modules/es.array.iterator.js");
require("core-js/modules/es.promise.js");
require("core-js/modules/es.regexp.exec.js");
Object.defineProperty(exports, "__esModule", {
  value: true
});
exports.ObjectList = exports.FETCH_FORMAT_OBJECT_LIST = void 0;
function _defineProperty2() {
  const data = _interopRequireDefault(require("@babel/runtime/helpers/defineProperty"));
  _defineProperty2 = function () {
    return data;
  };
  return data;
}
function _tarStream() {
  const data = _interopRequireDefault(require("tar-stream"));
  _tarStream = function () {
    return data;
  };
  return data;
}
function _pMap() {
  const data = _interopRequireDefault(require("p-map"));
  _pMap = function () {
    return data;
  };
  return data;
}
function _stream() {
  const data = require("stream");
  _stream = function () {
    return data;
  };
  return data;
}
function _() {
  const data = require(".");
  _ = function () {
    return data;
  };
  return data;
}
function _bitObjectList() {
  const data = require("./bit-object-list");
  _bitObjectList = function () {
    return data;
  };
  return data;
}
function _ref() {
  const data = _interopRequireDefault(require("./ref"));
  _ref = function () {
    return data;
  };
  return data;
}
function _logger() {
  const data = _interopRequireDefault(require("../../logger/logger"));
  _logger = function () {
    return data;
  };
  return data;
}
function _concurrency() {
  const data = require("../../utils/concurrency");
  _concurrency = function () {
    return data;
  };
  return data;
}
function _models() {
  const data = require("../models");
  _models = function () {
    return data;
  };
  return data;
}
function ownKeys(object, enumerableOnly) { var keys = Object.keys(object); if (Object.getOwnPropertySymbols) { var symbols = Object.getOwnPropertySymbols(object); enumerableOnly && (symbols = symbols.filter(function (sym) { return Object.getOwnPropertyDescriptor(object, sym).enumerable; })), keys.push.apply(keys, symbols); } return keys; }
function _objectSpread(target) { for (var i = 1; i < arguments.length; i++) { var source = null != arguments[i] ? arguments[i] : {}; i % 2 ? ownKeys(Object(source), !0).forEach(function (key) { (0, _defineProperty2().default)(target, key, source[key]); }) : Object.getOwnPropertyDescriptors ? Object.defineProperties(target, Object.getOwnPropertyDescriptors(source)) : ownKeys(Object(source)).forEach(function (key) { Object.defineProperty(target, key, Object.getOwnPropertyDescriptor(source, key)); }); } return target; }
function _asyncIterator(iterable) { var method, async, sync, retry = 2; for ("undefined" != typeof Symbol && (async = Symbol.asyncIterator, sync = Symbol.iterator); retry--;) { if (async && null != (method = iterable[async])) return method.call(iterable); if (sync && null != (method = iterable[sync])) return new AsyncFromSyncIterator(method.call(iterable)); async = "@@asyncIterator", sync = "@@iterator"; } throw new TypeError("Object is not async iterable"); }
function AsyncFromSyncIterator(s) { function AsyncFromSyncIteratorContinuation(r) { if (Object(r) !== r) return Promise.reject(new TypeError(r + " is not an object.")); var done = r.done; return Promise.resolve(r.value).then(function (value) { return { value: value, done: done }; }); } return AsyncFromSyncIterator = function (s) { this.s = s, this.n = s.next; }, AsyncFromSyncIterator.prototype = { s: null, n: null, next: function () { return AsyncFromSyncIteratorContinuation(this.n.apply(this.s, arguments)); }, return: function (value) { var ret = this.s.return; return void 0 === ret ? Promise.resolve({ value: value, done: !0 }) : AsyncFromSyncIteratorContinuation(ret.apply(this.s, arguments)); }, throw: function (value) { var thr = this.s.return; return void 0 === thr ? Promise.reject(value) : AsyncFromSyncIteratorContinuation(thr.apply(this.s, arguments)); } }, new AsyncFromSyncIterator(s); }
/**
 * when error occurred during streaming between HTTP server and client, there is no good way to
 * indicate this other than sending a new file with a special name and the error message.
 */
const TAR_STREAM_ERROR_FILENAME = '.BIT.ERROR';
/**
 * schema 1.0.0 - added the start and end file with basic info
 */
const OBJECT_LIST_CURRENT_SCHEMA = '1.0.0';
const TAR_STREAM_START_FILENAME = '.BIT.START';
const TAR_STREAM_END_FILENAME = '.BIT.END';
const FETCH_FORMAT_OBJECT_LIST = 'ObjectList';

/**
 * Stream.Readable that operates with objectMode, while each 'data' event emits one ObjectItem object.
 */
exports.FETCH_FORMAT_OBJECT_LIST = FETCH_FORMAT_OBJECT_LIST;
class ObjectList {
  constructor(objects = []) {
    this.objects = objects;
  }
  count() {
    return this.objects.length;
  }
  static mergeMultipleInstances(objectLists) {
    const objectList = new ObjectList();
    objectLists.forEach(objList => objectList.mergeObjectList(objList));
    return objectList;
  }
  mergeObjectList(objectList) {
    this.addIfNotExist(objectList.objects);
  }
  static fromJsonString(jsonStr) {
    const jsonParsed = JSON.parse(jsonStr);
    if (!Array.isArray(jsonParsed)) {
      throw new Error(`fromJsonString expect an array, got ${typeof jsonParsed}`);
    }
    jsonParsed.forEach(obj => {
      obj.ref = new (_ref().default)(obj.ref.hash);
      obj.buffer = Buffer.from(obj.buffer);
    });
    return new ObjectList(jsonParsed);
  }
  toJsonString() {
    return JSON.stringify(this.objects);
  }
  toTar() {
    const pack = _tarStream().default.pack();
    this.objects.forEach(obj => {
      pack.entry({
        name: ObjectList.combineScopeAndHash(obj)
      }, obj.buffer);
    });
    pack.finalize();
    return pack;
  }
  toReadableStream() {
    return _stream().Readable.from(this.objects);
  }
  static async fromTar(packStream) {
    const extract = _tarStream().default.extract();
    const objectItems = await new Promise((resolve, reject) => {
      const objects = [];
      extract.on('entry', (header, stream, next) => {
        const data = [];
        stream.on('data', chunk => {
          data.push(chunk);
        });
        stream.on('end', () => {
          objects.push(_objectSpread(_objectSpread({}, ObjectList.extractScopeAndHash(header.name)), {}, {
            buffer: Buffer.concat(data)
          }));
          next(); // ready for next entry
        });

        stream.on('error', err => reject(err));
        stream.resume(); // just auto drain the stream
      });

      extract.on('finish', () => {
        resolve(objects);
      });
      packStream.pipe(extract);
    });
    return new ObjectList(objectItems);
  }
  static fromTarToObjectStream(packStream) {
    const passThrough = new (_stream().PassThrough)({
      objectMode: true
    });
    const extract = _tarStream().default.extract();
    let startData;
    let endData;
    extract.on('entry', (header, stream, next) => {
      const data = [];
      stream.on('data', chunk => {
        data.push(chunk);
      });
      stream.on('end', () => {
        const allData = Buffer.concat(data);
        if (header.name === TAR_STREAM_ERROR_FILENAME) {
          passThrough.emit('error', new Error(allData.toString()));
          return;
        }
        if (header.name === TAR_STREAM_START_FILENAME) {
          startData = JSON.parse(allData.toString());
          _logger().default.debug('fromTarToObjectStream, start getting data', startData);
          next();
          return;
        }
        if (header.name === TAR_STREAM_END_FILENAME) {
          endData = JSON.parse(allData.toString());
          _logger().default.debug('fromTarToObjectStream, finished getting data', endData);
          next();
          return;
        }
        passThrough.write(_objectSpread(_objectSpread({}, ObjectList.extractScopeAndHash(header.name)), {}, {
          buffer: allData
        }));
        next(); // ready for next entry
      });

      stream.on('error', err => {
        passThrough.emit('error', err);
      });
      stream.resume(); // just auto drain the stream
    });

    // not sure if needed
    extract.on('error', err => {
      passThrough.emit('error', err);
    });
    extract.on('finish', () => {
      var _startData;
      if (((_startData = startData) === null || _startData === void 0 ? void 0 : _startData.schema) === OBJECT_LIST_CURRENT_SCHEMA && !endData) {
        // wasn't able to find a better way to indicate whether the server aborted the request
        // see https://github.com/node-fetch/node-fetch/issues/1117
        passThrough.emit('error', new Error(`server terminated the stream unexpectedly (metadata: ${JSON.stringify(startData)})`));
      }
      passThrough.end();
    });
    (0, _stream().pipeline)(packStream, extract, err => {
      if (err) {
        _logger().default.error('fromTarToObjectStream, pipeline', err);
        passThrough.emit('error', err);
      } else {
        _logger().default.debug('fromTarToObjectStream, pipeline is completed');
      }
    });
    return passThrough;
  }
  static fromObjectStreamToTar(readable, scopeName) {
    const pack = _tarStream().default.pack();
    const startFile = {
      schema: OBJECT_LIST_CURRENT_SCHEMA,
      scopeName
    };
    _logger().default.debug('fromObjectStreamToTar, start sending data', startFile);
    pack.entry({
      name: TAR_STREAM_START_FILENAME
    }, JSON.stringify(startFile));
    let numOfFiles = 0;
    readable.on('data', obj => {
      numOfFiles += 1;
      pack.entry({
        name: ObjectList.combineScopeAndHash(obj)
      }, obj.buffer);
    });
    readable.on('end', () => {
      const endFile = {
        numOfFiles,
        scopeName
      };
      _logger().default.debug('fromObjectStreamToTar, finished sending data', endFile);
      pack.entry({
        name: TAR_STREAM_END_FILENAME
      }, JSON.stringify(endFile));
      pack.finalize();
    });
    readable.on('error', err => {
      const errorMessage = err.message || `unexpected error (${err.name})`;
      _logger().default.error(`ObjectList.fromObjectStreamToTar, streaming an error as a file`, err);
      pack.entry({
        name: TAR_STREAM_ERROR_FILENAME
      }, errorMessage);
      pack.finalize();
    });
    return pack;
  }
  static async fromReadableStream(readable) {
    const objectItems = [];
    var _iteratorAbruptCompletion = false;
    var _didIteratorError = false;
    var _iteratorError;
    try {
      for (var _iterator = _asyncIterator(readable), _step; _iteratorAbruptCompletion = !(_step = await _iterator.next()).done; _iteratorAbruptCompletion = false) {
        const obj = _step.value;
        objectItems.push(obj);
      }
    } catch (err) {
      _didIteratorError = true;
      _iteratorError = err;
    } finally {
      try {
        if (_iteratorAbruptCompletion && _iterator.return != null) {
          await _iterator.return();
        }
      } finally {
        if (_didIteratorError) {
          throw _iteratorError;
        }
      }
    }
    return new ObjectList(objectItems);
  }

  /**
   * the opposite of this.combineScopeAndHash
   */
  static extractScopeAndHash(name) {
    const nameSplit = name.split('/');
    const hasScope = nameSplit.length > 1;
    return {
      scope: hasScope ? nameSplit[0] : undefined,
      ref: new (_ref().default)(hasScope ? nameSplit[1] : nameSplit[0])
    };
  }
  /**
   * the opposite of this.extractScopeAndHash
   */
  static combineScopeAndHash(objectItem) {
    const scope = objectItem.scope ? `${objectItem.scope}/` : '';
    return `${scope}${objectItem.ref.hash}`;
  }
  addIfNotExist(objectItems) {
    objectItems.forEach(objectItem => {
      const exists = this.objects.find(object => object.ref.isEqual(objectItem.ref) && object.scope === objectItem.scope);
      if (!exists) {
        this.objects.push(objectItem);
      }
    });
  }
  async toBitObjects() {
    const concurrency = (0, _concurrency().concurrentIOLimit)();
    const bitObjects = await (0, _pMap().default)(this.objects, object => _().BitObject.parseObject(object.buffer), {
      concurrency
    });
    return new (_bitObjectList().BitObjectList)(bitObjects);
  }
  static async fromBitObjects(bitObjects) {
    const concurrency = (0, _concurrency().concurrentIOLimit)();
    const objectItems = await (0, _pMap().default)(bitObjects, async obj => ({
      ref: obj.hash(),
      buffer: await obj.compress(),
      type: obj.getType()
    }), {
      concurrency
    });
    return new ObjectList(objectItems);
  }
  addScopeName(scopeName) {
    this.objects.forEach(object => {
      object.scope = scopeName;
    });
  }
  splitByScopeName() {
    const objectListPerScope = {};
    this.objects.forEach(obj => {
      if (obj.type === _models().ExportMetadata.name) {
        return; // no scope for this type. it's general for all export data from all scopes
      }

      if (!obj.scope) {
        throw new Error(`ObjectList: unable to split by scopeName, the scopeName is missing for ${obj.ref.hash}`);
      }
      if (objectListPerScope[obj.scope]) {
        objectListPerScope[obj.scope].addIfNotExist([obj]);
      } else {
        objectListPerScope[obj.scope] = new ObjectList([obj]);
      }
    });
    return objectListPerScope;
  }

  /**
   * helps debugging
   */
  toConsoleLog() {
    console.log(this.objects.map(o => o.ref.hash).join('\n')); // eslint-disable-line no-console
  }
}
exports.ObjectList = ObjectList;